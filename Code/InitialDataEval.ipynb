{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917391bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ae0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training dataset\n",
    "train = pd.read_csv(\"../../Data/RawDataCsvFormat/train.csv\",index_col=0)\n",
    "# Read in testing dataset\n",
    "test = pd.read_csv(\"../../Data/RawDataCsvFormat/test.csv\",index_col=0)\n",
    "# Test has an extra index column read in from the file. Need to remove it\n",
    "test = test.iloc[:,1:]\n",
    "#Read in development set\n",
    "dev = pd.read_csv(\"../../Data/RawDataCsvFormat/dev.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9588d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the proportion of the rows in the different datasets containing NA/Null values\n",
    "train_na_prop = len(train.loc[train.isnull().any(axis=1)])/len(train)\n",
    "dev_na_prop = len(dev.loc[dev.isnull().any(axis=1)])/len(dev)\n",
    "test_na_prop = len(test.loc[test.isnull().any(axis=1)])/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133127c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of rows with NA/Null feature value in Training Set:\n",
      "0.1990439381611066\n",
      "Proportion of rows with NA/Null feature value in Development Set:\n",
      "0.20475020475020475\n",
      "Proportion of rows with NA/Null feature value in Testing Set:\n",
      "0.18461538461538463\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of rows with NA/Null feature value in Training Set:\")\n",
    "print(train_na_prop)\n",
    "print(\"Proportion of rows with NA/Null feature value in Development Set:\")\n",
    "print(dev_na_prop)\n",
    "print(\"Proportion of rows with NA/Null feature value in Testing Set:\")\n",
    "print(test_na_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adb0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at proportions of missing values in each column of each of the datasets\n",
    "train_col_propNA = train.isnull().sum()/len(train)\n",
    "dev_col_propNA = dev.isnull().sum()/len(dev)\n",
    "test_col_propNA = test.isnull().sum()/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8d52e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of values in each column that are NA/Null for the Training Set:\n",
      "claim_id          0.000000\n",
      "claim             0.000814\n",
      "date_published    0.197010\n",
      "explanation       0.000814\n",
      "fact_checkers     0.001119\n",
      "main_text         0.002644\n",
      "sources           0.002848\n",
      "label             0.002746\n",
      "subjects          0.002848\n",
      "dtype: float64\n",
      "Proportion of values in each column that are NA/Null for the Development Set:\n",
      "claim_id          0.000000\n",
      "claim             0.001638\n",
      "date_published    0.200655\n",
      "explanation       0.001638\n",
      "fact_checkers     0.003276\n",
      "main_text         0.003276\n",
      "sources           0.004095\n",
      "label             0.004914\n",
      "subjects          0.005733\n",
      "dtype: float64\n",
      "Proportion of values in each column that are NA/Null for the Testing Set:\n",
      "claim_id          0.000000\n",
      "claim             0.000000\n",
      "date_published    0.182996\n",
      "explanation       0.000000\n",
      "fact_checkers     0.000000\n",
      "main_text         0.000000\n",
      "sources           0.000000\n",
      "label             0.001619\n",
      "subjects          0.001619\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of values in each column that are NA/Null for the Training Set:\")\n",
    "print(train_col_propNA)\n",
    "print(\"Proportion of values in each column that are NA/Null for the Development Set:\")\n",
    "print(dev_col_propNA)\n",
    "print(\"Proportion of values in each column that are NA/Null for the Testing Set:\")\n",
    "print(test_col_propNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae9799",
   "metadata": {},
   "source": [
    "'date_published' is the only feature suffering from a high number of Null values, and it is consistent in the proportion missing throughout each of the datasets. This is something I will take note of for later on, as I begin testing and parameterizing models, because a significant number of missing values always has the potential to cause confusion when evaluating and interpreting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4d60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the label distributions for each of these three data sets\n",
    "train_labels = train.loc[:,\"label\"]\n",
    "dev_labels = dev.loc[:,\"label\"]\n",
    "test_labels = test.loc[:,\"label\"]\n",
    "# Look at the unique values in each of the datasets\n",
    "train_lab_names = train_labels.unique()\n",
    "dev_lab_names = dev_labels.unique()\n",
    "test_lab_names = test_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1875d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['false' 'mixture' 'true' 'unproven' nan 'snopes']\n",
      "['unproven' 'true' 'false' 'mixture' nan\n",
      " 'National, Candidate Biography, Donald Trump, ']\n",
      "['false' 'true' 'unproven' 'mixture' nan]\n"
     ]
    }
   ],
   "source": [
    "# Know from dataset description, labels are {true,false,mixture,unproven} and possibly nan\n",
    "print(train_lab_names)\n",
    "print(dev_lab_names)\n",
    "print(test_lab_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723decbb",
   "metadata": {},
   "source": [
    "The test label names align with the dataset description, other than the need to look at why so many label values are read as 'nan' to confirm that is not a mistake (or misalignment) with how the data was read in or how the data was stored. Sacrificing labeled data that could be recovered can be a huge mistake, labeled data is scarce in general, and generative models almost always perform better when given more valid to learn from.\n",
    "\n",
    "The development set label names have 'snopes' in them for some reason. This is likely a mistake with the tabs in the .tsv file, so it was put in the wrong column and an easy fix. snopes.com is one of the sources for this data, so it must've gotten shifted into this column because of some white space mixup when reading and writing files multiple times. This is another indicator it is important to examine these nan values in the label columns, these little problems arise all the time and are important to think about with every dataset.\n",
    "\n",
    "The training set seems to have the exact same problem as the development set, but the value 'National, Candidate Biography, Donald Trump, ' seems more like it belongs in the subjects column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226eda2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
